#!/usr/bin/env python3
"""
Demo de las optimizaciones implementadas

Este script demuestra las mejoras de rendimiento y progreso en tiempo real
implementadas en el sistema de evaluaciones y pruebas.
"""

import asyncio
import time

from scripts.evaluacion_automatica import EvaluacionAutomatica
from scripts.update_test_results import main as run_optimized_tests
from scripts.update_test_results import set_progress_callback


def demo_progress_callback(progress_info):
    """Callback de demostraci√≥n para mostrar progreso"""
    print(
        f"\r[{progress_info.get('progress_percentage', 0):3.0f}%] "
        f"{progress_info.get('message', 'Procesando...')} "
        f"({progress_info.get('completed', 0)}/{progress_info.get('total', 0)}) "
        f"ETA: {progress_info.get('eta_formatted', 'N/A')}",
        end="",
        flush=True,
    )


async def demo_evaluaciones_optimizadas():
    """Demuestra las evaluaciones optimizadas con progreso"""
    print("\n=== DEMO: Evaluaciones Autom√°ticas Optimizadas ===")
    print("Caracter√≠sticas implementadas:")
    print("‚úì Paralelizaci√≥n con control de concurrencia")
    print("‚úì Cache de resultados para evitar re-evaluaciones")
    print("‚úì Progreso en tiempo real con ETA")
    print("‚úì Manejo robusto de errores")
    print("‚úì M√©tricas de rendimiento detalladas")

    # Crear instancia optimizada
    evaluacion = EvaluacionAutomatica(
        max_concurrent_requests=3, enable_cache=True  # Limitar concurrencia para demo
    )

    # Configurar callback de progreso
    evaluacion.set_progress_callback(demo_progress_callback)

    print("\nüöÄ Iniciando evaluaciones...")
    start_time = time.time()

    try:
        # Ejecutar evaluaci√≥n completa
        resultado = await evaluacion.ejecutar_evaluacion_completa("groq")

        end_time = time.time()
        print(
            f"\n\n‚úÖ Evaluaciones completadas en {end_time - start_time:.2f} segundos"
        )

        # Mostrar resumen de resultados
        print("\nüìä Resumen de resultados:")
        if "configuracion" in resultado:
            config = resultado["configuracion"]
            print(f"   ‚Ä¢ Modelo evaluado: {config.get('modelo', 'N/A')}")
            print(
                f"   ‚Ä¢ Concurrencia m√°xima: {config.get('max_concurrent_requests', 'N/A')}"
            )
            print(f"   ‚Ä¢ Cache habilitado: {config.get('enable_cache', 'N/A')}")

        if "metricas" in resultado:
            metricas = resultado["metricas"]
            print(f"   ‚Ä¢ Tiempo total: {metricas.get('tiempo_total_segundos', 0):.2f}s")
            print(
                f"   ‚Ä¢ Prompts por segundo: {metricas.get('prompts_por_segundo', 0):.2f}"
            )
            print(f"   ‚Ä¢ Hits de cache: {metricas.get('cache_hits', 0)}")
            print(
                f"   ‚Ä¢ Eficiencia de cache: {metricas.get('cache_efficiency', 0):.1f}%"
            )

        if "resumen_general" in resultado:
            resumen = resultado["resumen_general"]
            print(
                f"   ‚Ä¢ Puntuaci√≥n promedio: {resumen.get('puntuacion_promedio', 0):.2f}"
            )
            print(
                f"   ‚Ä¢ Tiempo promedio de respuesta: {resumen.get('tiempo_promedio_respuesta', 0):.2f}s"
            )

    except Exception as e:
        print(f"\n‚ùå Error durante las evaluaciones: {e}")


def demo_pruebas_optimizadas():
    """Demuestra las pruebas optimizadas con progreso"""
    print("\n\n=== DEMO: Pruebas Optimizadas ===")
    print("Caracter√≠sticas implementadas:")
    print("‚úì Ejecuci√≥n paralela de pruebas")
    print("‚úì Progreso en tiempo real")
    print("‚úì Timeout configurable")
    print("‚úì Generaci√≥n autom√°tica de reportes")
    print("‚úì Cache de resultados")

    print("\nüß™ Iniciando pruebas optimizadas...")

    # Configurar callback de progreso
    set_progress_callback(demo_progress_callback)

    start_time = time.time()

    try:
        # Ejecutar pruebas optimizadas
        resultado = run_optimized_tests()

        end_time = time.time()
        print(f"\n\n‚úÖ Pruebas completadas en {end_time - start_time:.2f} segundos")

        # Mostrar resumen
        print("\nüìä Resumen de pruebas:")
        print(f"   ‚Ä¢ √âxito: {resultado.get('success', False)}")
        print(f"   ‚Ä¢ Pasos completados: {resultado.get('steps_completed', 0)}")

        if "metrics" in resultado:
            metrics = resultado["metrics"]
            print(f"   ‚Ä¢ Tiempo total: {metrics.get('total_time', 0):.2f}s")
            print(f"   ‚Ä¢ Tiempo de parsing: {metrics.get('parse_time', 0):.2f}s")
            print(f"   ‚Ä¢ Tiempo de cache: {metrics.get('cache_update_time', 0):.2f}s")
            print(
                f"   ‚Ä¢ Tiempo de reportes: {metrics.get('report_generation_time', 0):.2f}s"
            )

        if "errors" in resultado and resultado["errors"]:
            print(f"   ‚ö†Ô∏è  Errores encontrados: {len(resultado['errors'])}")
            for error in resultado["errors"][:3]:  # Mostrar solo los primeros 3
                print(f"      - {error}")

    except Exception as e:
        print(f"\n‚ùå Error durante las pruebas: {e}")


def mostrar_comparacion_rendimiento():
    """Muestra una comparaci√≥n te√≥rica del rendimiento"""
    print("\n\n=== COMPARACI√ìN DE RENDIMIENTO ===")
    print("\nüìà Mejoras implementadas:")
    print("\n1. PARALELIZACI√ìN:")
    print("   ‚Ä¢ Antes: Evaluaciones secuenciales (1 por vez)")
    print("   ‚Ä¢ Ahora: Hasta 5 evaluaciones concurrentes")
    print("   ‚Ä¢ Mejora estimada: 3-5x m√°s r√°pido")

    print("\n2. CACHE DE RESULTADOS:")
    print("   ‚Ä¢ Antes: Re-evaluaci√≥n completa en cada ejecuci√≥n")
    print("   ‚Ä¢ Ahora: Cache inteligente de respuestas")
    print("   ‚Ä¢ Mejora estimada: 50-90% reducci√≥n en tiempo para re-ejecuciones")

    print("\n3. PROGRESO EN TIEMPO REAL:")
    print("   ‚Ä¢ Antes: Sin feedback durante ejecuci√≥n")
    print("   ‚Ä¢ Ahora: Progreso detallado con ETA")
    print("   ‚Ä¢ Mejora: Mejor experiencia de usuario")

    print("\n4. OPTIMIZACIONES DE C√ìDIGO:")
    print("   ‚Ä¢ Async/await para operaciones I/O")
    print("   ‚Ä¢ Eliminaci√≥n de redundancias")
    print("   ‚Ä¢ Manejo eficiente de memoria")
    print("   ‚Ä¢ Mejora estimada: 20-40% reducci√≥n en uso de recursos")

    print("\n5. API MEJORADA:")
    print("   ‚Ä¢ Ejecuci√≥n en segundo plano")
    print("   ‚Ä¢ Endpoints de progreso en tiempo real")
    print("   ‚Ä¢ Mejor manejo de errores")
    print("   ‚Ä¢ Fallback autom√°tico a versi√≥n legacy")


async def main():
    """Funci√≥n principal de demostraci√≥n"""
    print("üéØ DEMOSTRACI√ìN DE OPTIMIZACIONES IMPLEMENTADAS")
    print("=" * 60)

    # Mostrar comparaci√≥n de rendimiento
    mostrar_comparacion_rendimiento()

    # Preguntar al usuario qu√© demo ejecutar
    print("\n\nüîß ¬øQu√© demostraci√≥n deseas ejecutar?")
    print("1. Evaluaciones autom√°ticas optimizadas")
    print("2. Pruebas optimizadas")
    print("3. Ambas")
    print("4. Solo mostrar informaci√≥n (sin ejecutar)")

    try:
        opcion = input("\nSelecciona una opci√≥n (1-4): ").strip()

        if opcion == "1":
            await demo_evaluaciones_optimizadas()
        elif opcion == "2":
            demo_pruebas_optimizadas()
        elif opcion == "3":
            await demo_evaluaciones_optimizadas()
            demo_pruebas_optimizadas()
        elif opcion == "4":
            print("\n‚úÖ Informaci√≥n mostrada. No se ejecutaron pruebas.")
        else:
            print("\n‚ùå Opci√≥n no v√°lida.")

    except KeyboardInterrupt:
        print("\n\n‚èπÔ∏è  Demo interrumpida por el usuario.")
    except Exception as e:
        print(f"\n‚ùå Error durante la demo: {e}")

    print("\n\nüéâ Demo completada. ¬°Gracias por probar las optimizaciones!")


if __name__ == "__main__":
    asyncio.run(main())
